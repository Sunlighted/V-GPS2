import tensorflow as tf
import os
import numpy as np
from octo.data.dataset import make_dataset_from_rlds 

# è®¾ç½® GPU
os.environ["CUDA_VISIBLE_DEVICES"] = "1"

def check_embeddings(dataset_kwargs):
    print(f"æ­£åœ¨åŠ è½½æ•°æ®é›†: {dataset_kwargs.get('name')} ...")
    
    dataset, _ = make_dataset_from_rlds(
        **dataset_kwargs, 
        train=True, 
        shuffle=False,
    )

    print("æ­£åœ¨æ£€æŸ¥ Language Embedding çš„æœ‰æ•ˆæ€§...")
    
    zero_vector_count = 0
    unique_vectors = set()
    total_count = 0
    
    # æˆ‘ä»¬åªæ£€æŸ¥å‰ 1000 æ¡ï¼Œä¸ç„¶å¤ªæ…¢
    limit = 1000
    
    for i, traj in enumerate(dataset.iterator()):
        if i >= limit: break
        
        # æå– embedding
        # ç»“æ„é€šå¸¸æ˜¯: traj['steps']['action']... ä½† make_dataset_from_rlds å¯èƒ½ä¼šæ‰å¹³åŒ–
        # é€šå¸¸ embedding æ˜¯ä¸€æ­¥ä¸€ä¸ª (512,)ï¼Œæˆ–è€…æ•´æ¡è½¨è¿¹å…±äº«
        
        emb = None
        
        # 1. å°è¯•ä» task (Metadata) è·å–
        if "task" in traj and "language_embedding" in traj["task"]:
            emb = traj["task"]["language_embedding"]
            
        # 2. å°è¯•ä» steps è·å– (æ¯ä¸€æ­¥éƒ½æœ‰)
        elif "language_embedding" in traj:
            emb = traj["language_embedding"]
            # å¦‚æœæ˜¯åºåˆ—ï¼Œå–ç¬¬ä¸€å¸§
            if len(emb.shape) > 1: 
                emb = emb[0] 
                
        if emb is not None:
            # è½¬ numpy
            if hasattr(emb, 'numpy'):
                emb = emb.numpy()
            
            # ç»Ÿè®¡
            total_count += 1
            
            # æ£€æŸ¥æ˜¯å¦å…¨é›¶
            if np.allclose(emb, 0):
                zero_vector_count += 1
            
            # æ£€æŸ¥å”¯ä¸€æ€§ (ä¸ºäº†é€Ÿåº¦ï¼Œåªhashå‰10ä½)
            # æŠŠ float è½¬æˆ string æ¥åš hash key
            emb_hash = str(emb[:10]) 
            unique_vectors.add(emb_hash)
            
            if i < 5:
                print(f"Traj {i} Emb First 5 dims: {emb[:5]}")
        else:
            print(f"Traj {i}: æ‰¾ä¸åˆ° Embedding å­—æ®µ")

    print("\n" + "="*50)
    print("ğŸ“Š Embedding å°¸æ£€æŠ¥å‘Š")
    print("="*50)
    print(f"æ£€æŸ¥æ ·æœ¬æ•°: {total_count}")
    print(f"å…¨é›¶å‘é‡æ•° (Zero Vectors): {zero_vector_count}")
    print(f"ç‹¬ç«‹å‘é‡æ•° (Unique Vectors): {len(unique_vectors)}")
    print("="*50)
    
    if len(unique_vectors) < 10 and total_count > 100:
        print("ç»“è®º: âš ï¸ å‡ ä¹æ‰€æœ‰ Embedding éƒ½æ˜¯ä¸€æ ·çš„ï¼è¿™è¯´æ˜å®ƒä»¬å¤§æ¦‚ç‡æ¥è‡ªç©ºå­—ç¬¦ä¸²ã€‚")
    elif zero_vector_count > total_count * 0.5:
        print("ç»“è®º: âš ï¸ å¤§éƒ¨åˆ† Embedding æ˜¯å…¨é›¶ï¼Œæ— æ•ˆã€‚")
    else:
        print("ç»“è®º: âœ… Embedding çœ‹èµ·æ¥æœ‰å¤šæ ·æ€§ï¼Œå¯èƒ½æ¯”æ–‡æœ¬å­—æ®µä¿å­˜å¾—å¥½ï¼Ÿ(è™½ç„¶ä¸å¤ªå¯èƒ½)")

if __name__ == "__main__":
    my_dataset_args = {
        "name": "bridge_dataset",
        "data_dir": "/data/Chenyang/OXE_download",
        "image_obs_keys": {"primary": "image_0"}, 
        "language_key": "language_instruction", 
    }

    check_embeddings(my_dataset_args)