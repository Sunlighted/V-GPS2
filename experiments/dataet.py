import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import numpy as np
from tqdm import tqdm
import os
import dlimp as dl

# è®¾ç½® GPUï¼Œæ ¹æ®ä½ çš„æƒ…å†µè°ƒæ•´
os.environ["CUDA_VISIBLE_DEVICES"] = "1"

# å¿…é¡»ç¡®ä¿è¿™ä¸€æ­¥å¼•ç”¨çš„æ˜¯ä½ åˆšåˆšä¿®æ”¹è¿‡ dataset.py çš„é‚£ä¸ªåŒ…
from octo.data.dataset import make_dataset_from_rlds 

def analyze_task_distribution(dataset_kwargs, top_k=20):
    """
    ç»Ÿè®¡å¹¶å¯è§†åŒ– RLDS æ•°æ®é›†ä¸­çš„ä»»åŠ¡åˆ†å¸ƒã€‚
    
    Args:
        dataset_kwargs (dict): ä¼ é€’ç»™ make_dataset_from_rlds çš„å‚æ•°å­—å…¸ã€‚
        top_k (int): å¯è§†åŒ–æ—¶æ˜¾ç¤ºçš„å‡ºç°é¢‘ç‡æœ€é«˜çš„ä»»åŠ¡æ•°é‡ã€‚
    """
    print(f"æ­£åœ¨åŠ è½½æ•°æ®é›†: {dataset_kwargs.get('name')} ...")
    
    # 1. åˆ›å»ºæ•°æ®é›†
    # æ³¨æ„ï¼šæˆ‘ä»¬è®¾ç½® shuffle=False ä»¥ä¾¿ç¡®å®šæ€§åœ°è¯»å–ï¼Œtrain=True è¯»å–è®­ç»ƒé›†
    # è¿™é‡Œçš„å…³é”®æ˜¯åªè·å– datasetï¼Œä¸éœ€è¦ dataset_statistics
    dataset, _ = make_dataset_from_rlds(
        **dataset_kwargs, 
        train=True, 
        shuffle=False,
        # ç¡®ä¿åŒ…å« language_keyï¼Œå¦åˆ™æ— æ³•æå–æŒ‡ä»¤
        # å‡è®¾ä½ çš„æ•°æ®é›†ä¸­è¯­è¨€é”®åä¸º 'language_instruction' æˆ– 'language'
        # å¦‚æœä½ çš„ dataset_kwargs é‡Œæ²¡æœ‰ language_keyï¼Œè¯·åŠ¡å¿…åœ¨è¿™é‡Œæ·»åŠ 
    )

    # 2. éå†æ•°æ®é›†å¹¶è®¡æ•°
    task_counter = Counter()
    total_trajectories = 0
    
    print("æ­£åœ¨éå†æ•°æ®é›†ç»Ÿè®¡ä»»åŠ¡åˆ†å¸ƒ (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
    
    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦ (å¦‚æœçŸ¥é“æ€»é•¿åº¦å¯ä»¥ä¼ å…¥ total)
    for traj in tqdm(dataset.iterator()):
        # æå–è¯­è¨€æŒ‡ä»¤
        # æ³¨æ„ï¼šRLDS ä¸­çš„å­—ç¬¦ä¸²é€šå¸¸æ˜¯ bytes ç±»å‹ï¼Œéœ€è¦è§£ç 
        if "language_instruction" in traj["task"]:
            # language_instruction åœ¨ trajectory çº§åˆ«é€šå¸¸æ˜¯ä¸€ä¸ªæ ‡é‡å­—ç¬¦ä¸²
            # ä½†æœ‰æ—¶ä¸ºäº†å¯¹é½å¯èƒ½ä¼šè¢« repeatï¼Œæˆ‘ä»¬åªå–ç¬¬ä¸€ä¸ª
            lang_instr = traj["task"]["language_instruction"]
            
            # å¦‚æœæ˜¯ Tensorï¼Œè½¬æ¢ä¸º numpy å¹¶è§£ç 
            if isinstance(lang_instr, tf.Tensor):
                lang_instr = lang_instr.numpy()
            
            # å¤„ç†å¯èƒ½çš„ bytes ç±»å‹
            if isinstance(lang_instr, bytes):
                lang_instr = lang_instr.decode('utf-8')
            elif isinstance(lang_instr, np.ndarray): 
                # å¦‚æœæ˜¯æ•°ç»„ï¼ˆå› ä¸º repeatï¼‰ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                if lang_instr.size > 1:
                    lang_instr = lang_instr[0]
                lang_instr = str(lang_instr) if not isinstance(lang_instr, bytes) else lang_instr.decode('utf-8')

            task_counter[lang_instr] += 1
        else:
            task_counter["<No Instruction>"] += 1
            
        total_trajectories += 1

    # 3. æ‰“å°ç»Ÿè®¡æ‘˜è¦
    unique_tasks = len(task_counter)
    print("\n" + "="*40)
    print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡æ‘˜è¦")
    print(f"="*40)
    print(f"æ€»è½¨è¿¹æ•° (Total Trajectories): {total_trajectories}")
    print(f"ç‹¬ç«‹ä»»åŠ¡æ•° (Unique Tasks): {unique_tasks}")
    print(f"="*40)

    # 4. å¯è§†åŒ–
    sns.set_theme(style="whitegrid")
    plt.figure(figsize=(16, 12))

    # å­å›¾ 1: Top-K ä»»åŠ¡é¢‘ç‡
    plt.subplot(2, 1, 1)
    
    most_common = task_counter.most_common(top_k)
    tasks, counts = zip(*most_common)
    
    sns.barplot(x=list(counts), y=list(tasks), palette="viridis", hue=list(tasks), legend=False)
    plt.title(f"Top {top_k} Most Frequent Tasks (by Trajectory Count)", fontsize=15)
    plt.xlabel("Number of Trajectories")
    plt.ylabel("Language Instruction")

    # å­å›¾ 2: ä»»åŠ¡é¢‘ç‡åˆ†å¸ƒ
    plt.subplot(2, 1, 2)
    all_counts = list(task_counter.values())
    sns.histplot(all_counts, bins=50, kde=False, color="skyblue")
    plt.title("Distribution of Trajectory Counts per Task", fontsize=15)
    plt.xlabel("Number of Trajectories per Task")
    plt.ylabel("Number of Unique Tasks")
    plt.yscale('log') 

    plt.tight_layout()
    
    # --- æ–°å¢ï¼šä¿å­˜å›¾ç‰‡ ---
    # save_path å¯ä»¥æ˜¯ 'task_distribution.png' æˆ– 'task_distribution.pdf'
    save_path = "task_distribution_bridge.png" 
    print(f"æ­£åœ¨ä¿å­˜ç»Ÿè®¡å›¾åˆ°: {save_path}")
    
    # bbox_inches='tight' ç¡®ä¿é•¿çš„è¯­è¨€æŒ‡ä»¤ä¸ä¼šè¢«è¾¹ç¼˜è£å‰ªæ‰
    # dpi=300 ä¿è¯å›¾ç‰‡æ¸…æ™°åº¦è¶³å¤Ÿç”¨äºè®ºæ–‡æˆ–æŠ¥å‘Š
    plt.savefig(save_path, dpi=300, bbox_inches='tight') 
    
    plt.show() # æ˜¾ç¤ºå›¾ç‰‡
    
    # è®°å¾—å…³é—­ plot ä»¥é‡Šæ”¾å†…å­˜ï¼Œç‰¹åˆ«æ˜¯å¦‚æœä½ åœ¨å¾ªç¯ä¸­ç”»å›¾çš„è¯
    plt.close()
    
    return task_counter

def save_detailed_stats(task_counter, filename="task_distribution_bridge.txt"):
    """
    å°†ä»»åŠ¡ç»Ÿè®¡ç»“æœä¿å­˜åˆ° txt æ–‡ä»¶ï¼Œæ–¹ä¾¿æŸ¥é˜…é•¿å°¾åˆ†å¸ƒã€‚
    """
    sorted_tasks = task_counter.most_common()
    
    print(f"æ­£åœ¨ä¿å­˜ä»»åŠ¡ç»Ÿè®¡åˆ° {filename} ...")
    
    with open(filename, "w", encoding="utf-8") as f:
        # å†™å…¥å¤´éƒ¨ç»Ÿè®¡
        max_task, max_count = sorted_tasks[0]
        min_task, min_count = sorted_tasks[-1]
        unique_tasks = len(sorted_tasks)
        
        f.write("="*80 + "\n")
        f.write(f"DATASET STATISTICS SUMMARY\n")
        f.write(f"Total Unique Tasks: {unique_tasks}\n")
        f.write(f"Max Trajectories:   {max_count} (Task: {max_task})\n")
        f.write(f"Min Trajectories:   {min_count} (Task: {min_task})\n")
        f.write("="*80 + "\n\n")
        
        # å†™å…¥åˆ—è¡¨
        f.write(f"{'Rank':<6} | {'Count':<8} | {'Language Instruction'}\n")
        f.write("-" * 100 + "\n")
        
        single_traj_count = 0
        
        for rank, (task, count) in enumerate(sorted_tasks, 1):
            f.write(f"{rank:<6} | {count:<8} | {task}\n")
            if count == 1:
                single_traj_count += 1
        
        f.write("\n" + "="*80 + "\n")
        f.write(f"Single Trajectory Tasks: {single_traj_count} / {unique_tasks} ({single_traj_count/unique_tasks:.1%})\n")

    print(f"âœ… ä¿å­˜å®Œæˆï¼è¯·æŸ¥çœ‹æ–‡ä»¶: {filename}")
    print(f"âš ï¸ åªæœ‰ 1 æ¡è½¨è¿¹çš„ä»»åŠ¡å æ¯”: {single_traj_count/unique_tasks:.1%}")

# --- ä½¿ç”¨ç¤ºä¾‹ ---
if __name__ == "__main__":
    # é…ç½®ä½ çš„æ•°æ®é›†å‚æ•°
    # è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ data_dir å’Œ name
    my_dataset_args = {
        "name": "bridge_dataset",
        "data_dir": "/data/Chenyang/OXE_download",
        "image_obs_keys": {"primary": "image_0"}, 
        "language_key": "language_instruction", # å…³é”®å‚æ•°
    }

    # è¿è¡Œåˆ†æ
    counter = analyze_task_distribution(my_dataset_args)
    sorted_list = save_detailed_stats(counter)